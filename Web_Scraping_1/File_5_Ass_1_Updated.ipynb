{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sol.1 - Displaying all the header tags from wikipedia.org and make data frame."
      ],
      "metadata": {
        "id": "cNUHSrQqzTeQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFjS8aEmzMJs",
        "outputId": "02e4723d-18b4-4217-d10d-5a65b236d235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Header\n",
            "0                      Main Page\n",
            "1           Welcome to Wikipedia\n",
            "2  From today's featured article\n",
            "3               Did you know ...\n",
            "4                    In the news\n",
            "5                    On this day\n",
            "6       Today's featured picture\n",
            "7       Other areas of Wikipedia\n",
            "8    Wikipedia's sister projects\n",
            "9            Wikipedia languages\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Make a GET request to the Wikipedia homepage\n",
        "response = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all the header tags (h1, h2, h3, etc.) using BeautifulSoup's find_all method\n",
        "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "\n",
        "# Create a list to store the header tag text\n",
        "header_text = []\n",
        "\n",
        "# Loop through the header tags and append the text to the header_text list\n",
        "for tag in header_tags:\n",
        "    header_text.append(tag.text)\n",
        "\n",
        "# Create a pandas DataFrame from the header_text list\n",
        "df = pd.DataFrame(header_text, columns=['Header'])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol.2 - Displaying IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) and make data frame."
      ],
      "metadata": {
        "id": "2-AcNteP0aMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the URL and fetch the web page content\n",
        "url = 'https://www.imdb.com/chart/top/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all the movie rows in the HTML table and extract relevant data\n",
        "movie_rows = soup.select('table.chart tbody tr')\n",
        "movie_data = []\n",
        "for row in movie_rows:\n",
        "    title = row.find('td', class_='titleColumn').find('a').get_text().strip()\n",
        "    year = row.find('span', class_='secondaryInfo').get_text().strip('()')\n",
        "    rating = float(row.find('td', class_='ratingColumn imdbRating').get_text(strip=True))\n",
        "    movie_data.append({'Title': title, 'Year': year, 'Rating': rating})\n",
        "\n",
        "# Create a Pandas dataframe from the extracted movie data\n",
        "movie_df = pd.DataFrame(movie_data)\n",
        "print(movie_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63TJwvJ00mWo",
        "outputId": "185a6934-d9da-4e4f-bc66-c0f148cce94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        Title  Year  Rating\n",
            "0    The Shawshank Redemption  1994     9.2\n",
            "1               The Godfather  1972     9.2\n",
            "2             The Dark Knight  2008     9.0\n",
            "3       The Godfather Part II  1974     9.0\n",
            "4                12 Angry Men  1957     9.0\n",
            "..                        ...   ...     ...\n",
            "245            The Iron Giant  1999     8.0\n",
            "246                  The Help  2011     8.0\n",
            "247                   Aladdin  1992     8.0\n",
            "248               Dersu Uzala  1975     8.0\n",
            "249        Dances with Wolves  1990     8.0\n",
            "\n",
            "[250 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol.3 - Displaying IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
      ],
      "metadata": {
        "id": "9X7OijUodGoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# send a request to the website and get the HTML content\n",
        "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
        "response = requests.get(url)\n",
        "\n",
        "# parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# find all the movie containers\n",
        "movie_containers = soup.find_all('td', class_='titleColumn')\n",
        "\n",
        "# create empty lists to store movie details\n",
        "names = []\n",
        "ratings = []\n",
        "years = []\n",
        "\n",
        "# loop through each movie container and extract details\n",
        "for container in movie_containers[:50]:\n",
        "    name = container.a.text\n",
        "    rating = container.parent.find('td', class_='ratingColumn imdbRating').strong.text\n",
        "    year = container.span.text.strip('()')\n",
        "    names.append(name)\n",
        "    ratings.append(float(rating))\n",
        "    years.append(int(year))\n",
        "\n",
        "# create a data frame using Pandas\n",
        "data = {'Name': names, 'Rating': ratings, 'Year': years}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# print the data frame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GZ6slZgdR-U",
        "outputId": "c4336040-1152-4c9e-8070-8fd513a1221b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   Name  Rating  Year\n",
            "0   Ramayana: The Legend of Prince Rama     8.6  1993\n",
            "1            Rocketry: The Nambi Effect     8.4  2022\n",
            "2                               Nayakan     8.4  1987\n",
            "3                              Gol Maal     8.4  1979\n",
            "4                           777 Charlie     8.4  2022\n",
            "5                     Pariyerum Perumal     8.4  2018\n",
            "6                            Anbe Sivam     8.4  2003\n",
            "7                      The World of Apu     8.4  1959\n",
            "8                              3 Idiots     8.4  2009\n",
            "9                     Manichithrathazhu     8.3  1993\n",
            "10                             Jai Bhim     8.3  2021\n",
            "11                                #Home     8.3  2021\n",
            "12                      Soorarai Pottru     8.3  2020\n",
            "13                         Black Friday     8.3  2004\n",
            "14                    Kumbalangi Nights     8.3  2019\n",
            "15                    C/o Kancharapalem     8.3  2018\n",
            "16                  Like Stars on Earth     8.3  2007\n",
            "17                             Kireedam     8.3  1989\n",
            "18                               Dangal     8.3  2016\n",
            "19                               Kaithi     8.3  2019\n",
            "20                               Jersey     8.3  2019\n",
            "21                                   96     8.3  2018\n",
            "22                            Mayabazar     8.2  1957\n",
            "23                            Natsamrat     8.2  2016\n",
            "24                               Asuran     8.2  2019\n",
            "25                           Drishyam 2     8.2  2021\n",
            "26                         Thevar Magan     8.2  1992\n",
            "27                           Sita Ramam     8.2  2022\n",
            "28                           Visaaranai     8.2  2015\n",
            "29                  Sarpatta Parambarai     8.2  2021\n",
            "30                           Thalapathi     8.2  1991\n",
            "31                         Nadodikkattu     8.2  1987\n",
            "32                      Pather Panchali     8.2  1955\n",
            "33                             Drishyam     8.2  2013\n",
            "34                         Thani Oruvan     8.2  2015\n",
            "35                   Jaane Bhi Do Yaaro     8.2  1983\n",
            "36                         Vada Chennai     8.2  2018\n",
            "37                            Aparajito     8.2  1956\n",
            "38                    Khosla Ka Ghosla!     8.2  2006\n",
            "39                         Sardar Udham     8.2  2021\n",
            "40                              Anniyan     8.2  2005\n",
            "41                           Raatchasan     8.1  2018\n",
            "42                        Chupke Chupke     8.1  1975\n",
            "43                   Gangs of Wasseypur     8.1  2012\n",
            "44                             Drishyam     8.1  2015\n",
            "45                             Mahanati     8.1  2018\n",
            "46                              Peranbu     8.1  2018\n",
            "47                       Bangalore Days     8.1  2014\n",
            "48                               Premam     8.1  2015\n",
            "49                                Satya     8.1  1998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol.4 - Displaying list of respected former presidents of India(i.e. Name , Term ofoffice) \n",
        "from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
      ],
      "metadata": {
        "id": "52z1faE8d5ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "presidents_table = soup.find(\"table\", {\"class\": \"table\"})\n",
        "rows = presidents_table.find_all(\"table\")[1:11]\n",
        "\n",
        "presidents_data = []\n",
        "for row in rows:\n",
        "    cols = row.find_all(\"td\")\n",
        "    name = cols[0].text.strip()\n",
        "    term = cols[1].text.strip()\n",
        "    presidents_data.append({\"Name\": name, \"Term\": term})\n",
        "\n",
        "df = pd.DataFrame(presidents_data)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "tSwdKtAGeEJT",
        "outputId": "09958639-e0c3-46bd-9afb-332d9d7b8cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8b503027661a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpresidents_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"table\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresidents_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpresidents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol.5 - Scraping cricket rankings from icc-cricket.com and making data frame for\n",
        "\n",
        "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
        "\n",
        "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
        "\n",
        "c) Top 10 ODI bowlers along with the records of their team andrating"
      ],
      "metadata": {
        "id": "4QDUPyfWhDIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Top 10 ODI Teams\n",
        "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "team_table = soup.find('table', {'class': 'table'})\n",
        "team_rows = team_table.find_all('tr')[1:11]\n",
        "team_data = []\n",
        "for row in team_rows:\n",
        "    cols = row.find_all('td')\n",
        "    team = cols[1].text.strip(\"\\n\")\n",
        "    matches = cols[2].text.strip()\n",
        "    points = cols[3].text.strip()\n",
        "    rating = cols[4].text.strip()\n",
        "    team_data.append([team, matches, points, rating])\n",
        "team_df = pd.DataFrame(team_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
        "print(\"Top 10 ODI Teams\")\n",
        "print(team_df)\n",
        "\n",
        "# Top 10 ODI Batsmen\n",
        "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "batsman_table = soup.find('table', {'class': 'table'})\n",
        "batsman_rows = batsman_table.find_all('tr')[1:11]\n",
        "batsman_data = []\n",
        "for row in batsman_rows:\n",
        "    cols = row.find_all('td')\n",
        "    batsman = cols[1].text.strip()\n",
        "    team = cols[2].text.strip()\n",
        "    rating = cols[3].text.strip()\n",
        "    batsman_data.append([batsman, team, rating])\n",
        "batsman_df = pd.DataFrame(batsman_data, columns=['Batsman', 'Team', 'Rating'])\n",
        "print(\"\\nTop 10 ODI Batsmen\")\n",
        "print(batsman_df)\n",
        "\n",
        "# Top 10 ODI Bowlers\n",
        "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "bowler_table = soup.find('table', {'class': 'table'})\n",
        "bowler_rows = bowler_table.find_all('tr')[1:11]\n",
        "bowler_data = []\n",
        "for row in bowler_rows:\n",
        "    cols = row.find_all('td')\n",
        "    bowler = cols[1].text.strip()\n",
        "    team = cols[2].text.strip()\n",
        "    rating = cols[3].text.strip()\n",
        "    bowler_data.append([bowler, team, rating])\n",
        "bowler_df = pd.DataFrame(bowler_data, columns=['Bowler', 'Team', 'Rating'])\n",
        "print(\"\\nTop 10 ODI Bowlers\")\n",
        "print(bowler_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tm3v1kvhdvz",
        "outputId": "cb5b556d-8c0f-4a88-86da-1caf3c96f9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 ODI Teams\n",
            "               Team Matches Points Rating\n",
            "0    Australia\\nAUS      35  3,965    113\n",
            "1   New Zealand\\nNZ      31  3,504    113\n",
            "2        India\\nIND      47  5,294    113\n",
            "3      England\\nENG      36  3,988    111\n",
            "4     Pakistan\\nPAK      25  2,649    106\n",
            "5  South Africa\\nSA      31  3,141    101\n",
            "6   Bangladesh\\nBAN      38  3,625     95\n",
            "7     Sri Lanka\\nSL      36  3,099     86\n",
            "8   West Indies\\nWI      43  3,105     72\n",
            "9  Afghanistan\\nAFG      20  1,419     71\n",
            "\n",
            "Top 10 ODI Batsmen\n",
            "                 Batsman Team Rating\n",
            "0             Babar Azam  PAK    887\n",
            "1  Rassie van der Dussen   SA    777\n",
            "2            Imam-ul-Haq  PAK    740\n",
            "3           Shubman Gill  IND    738\n",
            "4           David Warner  AUS    726\n",
            "5            Virat Kohli  IND    719\n",
            "6        Quinton de Kock   SA    718\n",
            "7           Rohit Sharma  IND    707\n",
            "8            Steve Smith  AUS    702\n",
            "9           Fakhar Zaman  PAK    699\n",
            "\n",
            "Top 10 ODI Bowlers\n",
            "             Bowler Team Rating\n",
            "0    Josh Hazlewood  AUS    705\n",
            "1       Trent Boult   NZ    694\n",
            "2    Mohammed Siraj  IND    691\n",
            "3    Mitchell Starc  AUS    686\n",
            "4        Matt Henry   NZ    676\n",
            "5       Rashid Khan  AFG    659\n",
            "6        Adam Zampa  AUS    652\n",
            "7    Shaheen Afridi  PAK    641\n",
            "8  Mujeeb Ur Rahman  AFG    637\n",
            "9   Shakib Al Hasan  BAN    636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol. 6 - Scraping cricket rankings from icc-cricket.com and making data frame for\n",
        "\n",
        "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
        "\n",
        "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
        "\n",
        "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
      ],
      "metadata": {
        "id": "Dvjhoaqph8uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Top 10 ODI Teams\n",
        "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "team_table = soup.find('table', {'class': 'table'})\n",
        "team_rows = team_table.find_all('tr')[1:11]\n",
        "team_data = []\n",
        "for row in team_rows:\n",
        "    cols = row.find_all('td')\n",
        "    team = cols[1].text.strip()\n",
        "    matches = cols[2].text.strip()\n",
        "    points = cols[3].text.strip()\n",
        "    rating = cols[4].text.strip()\n",
        "    team_data.append([team, matches, points, rating])\n",
        "team_df = pd.DataFrame(team_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
        "print(\"Top 10 ODI Teams\")\n",
        "print(team_df)\n",
        "\n",
        "# Top 10 ODI Batsmen\n",
        "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "batsman_table = soup.find('table', {'class': 'table'})\n",
        "batsman_rows = batsman_table.find_all('tr')[1:11]\n",
        "batsman_data = []\n",
        "for row in batsman_rows:\n",
        "    cols = row.find_all('td')\n",
        "    batsman = cols[1].text.strip()\n",
        "    team = cols[2].text.strip()\n",
        "    rating = cols[3].text.strip()\n",
        "    batsman_data.append([batsman, team, rating])\n",
        "batsman_df = pd.DataFrame(batsman_data, columns=['Batsman', 'Team', 'Rating'])\n",
        "print(\"\\nTop 10 ODI Batsmen\")\n",
        "print(batsman_df)\n",
        "\n",
        "# Top 10 ODI Bowlers\n",
        "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "all_rounder_table = soup.find('table', {'class': 'table'})\n",
        "all_rounder_rows = all_rounder_table.find_all('tr')[1:11]\n",
        "all_rounder_data = []\n",
        "for row in all_rounder_rows:\n",
        "    cols = row.find_all('td')\n",
        "    all_rounder = cols[1].text.strip()\n",
        "    team = cols[2].text.strip()\n",
        "    rating = cols[3].text.strip()\n",
        "    all_rounder_data.append([all_rounder, team, rating])\n",
        "all_rounder_df = pd.DataFrame(all_rounder_data, columns=['All Rounder', 'Team', 'Rating'])\n",
        "print(\"\\nTop 10 ODI All Rounders\")\n",
        "print(all_rounder_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BPhas4lh2zC",
        "outputId": "d7baf355-cca9-44b5-8e06-be43990418a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 ODI Teams\n",
            "               Team Matches Points Rating\n",
            "0    Australia\\nAUS      21  3,603    172\n",
            "1      England\\nENG      28  3,342    119\n",
            "2  South Africa\\nSA      26  3,098    119\n",
            "3        India\\nIND      27  2,820    104\n",
            "4   New Zealand\\nNZ      25  2,553    102\n",
            "5   West Indies\\nWI      27  2,535     94\n",
            "6   Bangladesh\\nBAN      13    983     76\n",
            "7     Thailand\\nTHA      11    821     75\n",
            "8     Pakistan\\nPAK      27  1,678     62\n",
            "9     Sri Lanka\\nSL       8    353     44\n",
            "\n",
            "Top 10 ODI Batsmen\n",
            "               Batsman Team Rating\n",
            "0         Alyssa Healy  AUS    762\n",
            "1          Beth Mooney  AUS    754\n",
            "2      Laura Wolvaardt   SA    732\n",
            "3       Natalie Sciver  ENG    731\n",
            "4          Meg Lanning  AUS    717\n",
            "5     Harmanpreet Kaur  IND    716\n",
            "6      Smriti Mandhana  IND    714\n",
            "7  Chamari Athapaththu   SL    655\n",
            "8    Amy Satterthwaite   NZ    641\n",
            "9         Ellyse Perry  AUS    626\n",
            "\n",
            "Top 10 ODI All Rounders\n",
            "         All Rounder Team Rating\n",
            "0    Hayley Matthews   WI    373\n",
            "1     Natalie Sciver  ENG    371\n",
            "2       Ellyse Perry  AUS    366\n",
            "3     Marizanne Kapp   SA    349\n",
            "4        Amelia Kerr   NZ    336\n",
            "5      Deepti Sharma  IND    322\n",
            "6   Ashleigh Gardner  AUS    292\n",
            "7      Jess Jonassen  AUS    250\n",
            "8           Nida Dar  PAK    232\n",
            "9  Sophie Ecclestone  ENG    205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol.7 - Scraping news details from https://www.cnbc.com/world/?region=world and \n",
        "storing them in a data frame\n",
        "\n",
        "i) Headline\n",
        "\n",
        "ii) Time\n",
        "\n",
        "iii) News Link"
      ],
      "metadata": {
        "id": "SIXbAFSviTIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.cnbc.com/world/?region=world'\n",
        "page = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "headlines = []\n",
        "times = []\n",
        "links = []\n",
        "\n",
        "articles = soup.find_all('div', class_='Card-titleContainer')\n",
        "\n",
        "for article in articles:\n",
        "    headline = article.find('a', class_='Card-title')\n",
        "    if headline is not None:\n",
        "        headlines.append(headline.text.strip())\n",
        "        links.append(headline['href'])\n",
        "    else:\n",
        "        headlines.append('No headline')\n",
        "        links.append('No link')\n",
        "        \n",
        "    time = article.find('span', class_='Card-time')\n",
        "    if time is not None:\n",
        "        times.append(time.text.strip())\n",
        "    else:\n",
        "        times.append('No time')\n",
        "\n",
        "df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': links})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93p0rhpAiXtl",
        "outputId": "5de5fae9-ae15-4247-e507-b47d70cfe588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Headline     Time  \\\n",
            "0   Ukraine should use China as leverage to help w...  No time   \n",
            "1   Russian airstrikes in Ukrainian cities kill at...  No time   \n",
            "2   China just called Ukraine. The timing wasn't a...  No time   \n",
            "3   Powell duped by Russian pranksters claiming to...  No time   \n",
            "4   Moscow 'welcomes' China contacting Ukraine; Ky...  No time   \n",
            "5   EU agrees to decarbonize air travel with the '...  No time   \n",
            "6   Oliver Stone and Joshua Goldstein on their doc...  No time   \n",
            "7   Electric car sales in 2022 hit over 10 million...  No time   \n",
            "8   Oliver Stone and Joshua Goldstein on why gover...  No time   \n",
            "9   VC firms create $60 billion-plus climate tech ...  No time   \n",
            "10  Abortion pill mifepristone is banned or restri...  No time   \n",
            "11  Lilly says obesity drug tirzepatide resulted i...  No time   \n",
            "12  Eli Lilly misses on earnings but raises full-y...  No time   \n",
            "13  Mpox outbreak was wake-up call about need to p...  No time   \n",
            "14  Disney, Meta layoffs: These are the first move...  No time   \n",
            "15  Royal castles to luxury getaways: How to spend...  No time   \n",
            "16  New reports say business travel isn’t going ba...  No time   \n",
            "17  Beyond Kyoto: Japan recommends 11 lesser-known...  No time   \n",
            "18  Gen Zs don't have a lot of money, but they're ...  No time   \n",
            "19  What flight behavior is ‘unacceptable?' It may...  No time   \n",
            "20  Here's how to raise a kids with a ‘secure’ att...  No time   \n",
            "21  TikTokers are using beef tallow to treat acne:...  No time   \n",
            "22  This 28-year-old pays $62 a month to live in a...  No time   \n",
            "23  I live in a $62/month dumpster that I built fo...  No time   \n",
            "24  This 25-year-old makes $200/hour without a bac...  No time   \n",
            "\n",
            "                                            News Link  \n",
            "0   https://www.cnbc.com/2023/04/28/ukraine-should...  \n",
            "1   https://www.cnbc.com/2023/04/28/ukraine-war-li...  \n",
            "2   https://www.cnbc.com/2023/04/28/why-did-china-...  \n",
            "3   https://www.cnbc.com/2023/04/27/powell-duped-b...  \n",
            "4   https://www.cnbc.com/2023/04/27/ukraine-war-li...  \n",
            "5   https://www.cnbc.com/2023/04/26/air-travel-eu-...  \n",
            "6   https://www.cnbc.com/video/2023/04/28/oliver-s...  \n",
            "7   https://www.cnbc.com/2023/04/26/electric-car-s...  \n",
            "8   https://www.cnbc.com/video/2023/04/28/oliver-s...  \n",
            "9   https://www.cnbc.com/2023/04/25/vc-firms-form-...  \n",
            "10  https://www.cnbc.com/2023/04/27/supreme-court-...  \n",
            "11  https://www.cnbc.com/2023/04/27/eli-lilly-weig...  \n",
            "12  https://www.cnbc.com/2023/04/27/eli-lilly-lly-...  \n",
            "13  https://www.cnbc.com/2023/04/25/mpox-shows-sma...  \n",
            "14  https://www.cnbc.com/2023/04/24/disney-meta-la...  \n",
            "15  https://www.cnbc.com/2023/04/26/how-to-spend-k...  \n",
            "16  https://www.cnbc.com/2023/04/24/is-business-tr...  \n",
            "17  https://www.cnbc.com/2023/04/19/best-places-to...  \n",
            "18  https://www.cnbc.com/2023/04/17/gen-z-travel-t...  \n",
            "19  https://www.cnbc.com/2023/04/12/should-you-rec...  \n",
            "20  https://www.cnbc.com/2023/04/29/how-to-raise-a...  \n",
            "21  https://www.cnbc.com/2023/04/29/tiktokers-are-...  \n",
            "22  https://www.cnbc.com/2023/04/29/28-year-old-pa...  \n",
            "23  https://www.cnbc.com/video/2023/04/29/i-live-i...  \n",
            "24  https://www.cnbc.com/2023/04/29/25-year-old-no...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol.8 -  Scraping the details of most downloaded articles from AI in last 90\n",
        "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles and creating data frame that includes\n",
        "\n",
        "i) Paper Title\n",
        "\n",
        "ii) Authors\n",
        "\n",
        "iii) Published Date\n",
        "\n",
        "iv) Paper URL\n"
      ],
      "metadata": {
        "id": "XQyBCjZUjCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "title_list = []\n",
        "author_list = []  \n",
        "date_list = []\n",
        "url_list = []\n",
        "\n",
        "articles = soup.find_all('div', {'class': 'sc-orwwe2-3 jOMrrY'})\n",
        "for article in articles:\n",
        "    title = article.find('a', {'class': 'sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'})\n",
        "    author = article.find('p', {'class': 'sc-1w3fpd7-0 dnCnAO'})\n",
        "    date = article.find('p', {'class': 'sc-1thf9ly-2 dvggWt'})\n",
        "    url = 'https://www.sciencedirect.com/science/article/pii' + article.find('a', {'class': 'sc-5smygv-0 fIXTHm'})['href']\n",
        "    \n",
        "    title_list.append(title)\n",
        "    author_list.append(author)\n",
        "    date_list.append(date)\n",
        "    url_list.append(url)\n",
        "\n",
        "data = {'Paper Title': title_list, 'Authors': author_list, 'Published Date': date_list, 'Paper URL': url_list}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRX25_H-jDvp",
        "outputId": "58f418b2-8672-48c0-c51e-13cdd45fb1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Paper Title Authors Published Date  \\\n",
            "0        None    None           None   \n",
            "\n",
            "                                           Paper URL  \n",
            "0  https://www.sciencedirect.com/science/article/...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol. 9 - Scraping the below details from dineout.co.in and storing them in a data frame\n",
        "\n",
        "i) Restaurant name\n",
        "\n",
        "ii) Cuisine\n",
        "\n",
        "iii) Location\n",
        "\n",
        "iv) Ratings\n",
        "\n",
        "v) Image UR\n",
        "\n",
        "This code will only show output when executed in Indian region"
      ],
      "metadata": {
        "id": "OflDZdbLjkrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "restaurant_list = []\n",
        "cuisine_list = []\n",
        "location_list = []\n",
        "rating_list = []\n",
        "image_url_list = []\n",
        "\n",
        "restaurants = soup.find_all('div', {'class': 'restnt-card'})\n",
        "for restaurant in restaurants:\n",
        "    name = restaurant.find('div', {'class': 'restnt-name'}).text.strip()\n",
        "    cuisine = restaurant.find('div', {'class': 'restnt-cuisine'}).text.strip()\n",
        "    location = restaurant.find('div', {'class': 'restnt-loc'}).text.strip()\n",
        "    rating = restaurant.find('div', {'class': 'restnt-rating'}).text.strip()\n",
        "    image_url = restaurant.find('img')['data-original']\n",
        "    \n",
        "    restaurant_list.append(name)\n",
        "    cuisine_list.append(cuisine)\n",
        "    location_list.append(location)\n",
        "    rating_list.append(rating)\n",
        "    image_url_list.append(image_url)\n",
        "\n",
        "data = {'Restaurant Name': restaurant_list, 'Cuisine': cuisine_list, 'Location': location_list, 'Ratings': rating_list, 'Image URL': image_url_list}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_XYa9gbjmDB",
        "outputId": "853567ac-5598-4917-d439-9c01ab16e45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
            "Index: []\n"
          ]
        }
      ]
    }
  ]
}